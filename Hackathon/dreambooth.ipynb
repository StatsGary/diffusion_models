{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2e53fba-f8f9-4a40-aff2-7d8132d337f0",
   "metadata": {},
   "source": [
    "# DreamBooth Hackathon ğŸ†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e0c708-4d02-425d-8419-c1994c2fe6a9",
   "metadata": {},
   "source": [
    "Welcome to the DreamBooth Hackathon! In this competition, you'll **personalise a Stable Diffusion model by fine-tuning it on a handful of your own images.** To do so, we'll use a technique called [_DreamBooth_](https://arxiv.org/abs/2208.12242), which allows one to implant a subject (e.g. your pet or favourite dish) into the output domain of the model such that it can be synthesized with a _unique identifier_ in the prompt.\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66c2120-70f4-429d-8404-942f878c640e",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before diving into this notebook, you should read the:\n",
    "\n",
    "* [Unit 3 README](https://github.com/huggingface/diffusion-models-class/blob/main/unit3/README.md) that contains a deep dive into Stable Diffusion\n",
    "* DreamBooth [blog post](https://dreambooth.github.io/) to get a sense of what's possible with this technique\n",
    "* Hugging Face [blog post](https://huggingface.co/blog/dreambooth) on best practices for fine-tuning Stable Diffusion with DreamBooth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3557b558-77d0-42be-b515-c3ac762fdd71",
   "metadata": {},
   "source": [
    "ğŸš¨ **Note:** the code in **this notebook requires at least 14GB of GPU vRAM** and is a simplified version of the [official training script](https://github.com/huggingface/diffusers/tree/main/examples/dreambooth) provided in ğŸ¤— Diffusers. It produces decent models for most applications, but we recommend experimenting with the advanced features like class preservation loss & fine-tuning the text encoder if you have at least 24GB vRAM available. Check out the ğŸ¤— Diffusers [docs](https://huggingface.co/docs/diffusers/training/dreambooth) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d2ea8e-0021-4346-802b-37495b8e3889",
   "metadata": {},
   "source": [
    "## What is DreamBooth?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ec519c-6464-4842-87b3-c6d1c09bd338",
   "metadata": {},
   "source": [
    "DreamBooth is a technique to teach new concepts to Stable Diffusion using a specialized form of fine-tuning. If you're on Twitter or Reddit, you may have seen people using this technique to create (often hilarious) avatars of themselves. For example, here's what [Andrej Karpathy](https://karpathy.ai/) would look like as a cowboy (you may need to run the cell to see the output):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32638afb-70e1-4ba9-b0d1-ff51c9be840c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%html\n",
    "# <blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Stableboost auto-suggests a few hundred prompts by default but you can generate additional variations for any one prompt that seems to be giving fun/interesting results, or adjust it in any way: <a href=\"https://t.co/qWmadiXftP\">pic.twitter.com/qWmadiXftP</a></p>&mdash; Andrej Karpathy (@karpathy) <a href=\"https://twitter.com/karpathy/status/1600578187141840896?ref_src=twsrc%5Etfw\">December 7, 2022</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5929407a-c94c-4778-a184-8fac612a4ab7",
   "metadata": {},
   "source": [
    "The way DreamBooth works is as follows:\n",
    "\n",
    "* Collect around 10-20 input images of a subject (e.g. your dog) and define a unique identifier [V] the refers to the subject. This identifier is usually some made up word like `flffydog` which is implanted in different text prompts at inference time to place the subject in different contexts.\n",
    "* Fine-tune the diffusion model by providing the images together with a text prompt like \"A photo of a [V] dog\" that contains the unique identifier and class name (i.e. \"dog\" in this example)\n",
    "* (Optionally) Apply a special _class-specific prior preservation loss_, which leverages the semantic prior that the model has on the class and encourages it to generate diverse instances belong to the subject's class by injecting the class name in the text prompt. In practice, this step is only really needed for human faces and can be skipped for the themes we'll be exploring in this hackathon.\n",
    "\n",
    "An overview of the DreamBooth technique is shown in the image below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451b8f06-5dd4-47e2-8977-24cd1cd4b011",
   "metadata": {},
   "source": [
    "![](https://dreambooth.github.io/DreamBooth_files/high_level.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902ab068-35a8-4f43-b78a-b7c6e5133247",
   "metadata": {},
   "source": [
    "### What can DreamBooth do?\n",
    "\n",
    "Besides putting your subject in interesting locations, DreamBooth can be used for _**text-guided view synthesis**_, where the subject is viewed from different viewpoints as shown in the example below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60208483-ca9a-4a31-8a53-a583233cc612",
   "metadata": {},
   "source": [
    "![](https://dreambooth.github.io/DreamBooth_files/novel_views.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dff5c9-d879-45ec-9ede-0312009ac039",
   "metadata": {},
   "source": [
    "DreamBooth can also be used to modify properties of the subject, such as colour or mixing up animal species!\n",
    "\n",
    "![](https://dreambooth.github.io/DreamBooth_files/property_modification.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3034f9cd-208b-420b-8e4f-cacf2a5585b7",
   "metadata": {},
   "source": [
    "Now that we've seen some of the cool things DreamBooth can do, let's start training our own models!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2488819b-aa50-4c15-a65c-d492223c8417",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eb3232-8099-47cd-9bf0-eb78e88c1fec",
   "metadata": {},
   "source": [
    "If you're running this notebook on Google Colab or Kaggle, run the cell below to install the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66867656-b216-4ff3-8d31-103c23cf27b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting diffusers\n",
      "  Downloading diffusers-0.11.1-py3-none-any.whl (524 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m524.9/524.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers\n",
      "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting accelerate\n",
      "  Downloading accelerate-0.15.0-py3-none-any.whl (191 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m191.5/191.5 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ftfy\n",
      "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets\n",
      "  Downloading datasets-2.8.0-py3-none-any.whl (452 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m452.9/452.9 kB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpy\n",
      "  Downloading numpy-1.24.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting Pillow\n",
      "  Downloading Pillow-9.4.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m95.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2022.10.31-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (772 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m772.3/772.3 kB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.9.0-py3-none-any.whl (9.7 kB)\n",
      "Collecting requests\n",
      "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub>=0.10.0\n",
      "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m182.4/182.4 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting importlib-metadata\n",
      "  Downloading importlib_metadata-6.0.0-py3-none-any.whl (21 kB)\n",
      "Collecting tqdm>=4.27\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from transformers) (22.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m100.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (701 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m701.2/701.2 kB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torch>=1.4.0\n",
      "  Downloading torch-1.13.1-cp38-cp38-manylinux1_x86_64.whl (887.4 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m887.4/887.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from ftfy) (0.2.5)\n",
      "Collecting dill<0.3.7\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-1.5.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m102.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.0/132.0 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec[http]>=2021.11.1\n",
      "  Downloading fsspec-2022.11.0-py3-none-any.whl (139 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.5/139.5 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=6.0.0\n",
      "  Downloading pyarrow-10.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.0 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m36.0/36.0 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m213.0/213.0 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.3/121.3 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting charset-normalizer<3.0,>=2.0\n",
      "  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Collecting attrs>=17.3.0\n",
      "  Downloading attrs-22.2.0-py3-none-any.whl (60 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (262 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m262.1/262.1 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m161.3/161.3 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting typing-extensions>=3.7.4.3\n",
      "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from requests->diffusers) (2022.12.7)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m140.6/140.6 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4.0->accelerate) (0.37.1)\n",
      "Requirement already satisfied: setuptools in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4.0->accelerate) (65.5.0)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.11.0-py3-none-any.whl (6.6 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2022.7-py2.py3-none-any.whl (499 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m499.4/499.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: tokenizers, pytz, zipp, xxhash, urllib3, typing-extensions, tqdm, regex, pyyaml, Pillow, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, numpy, multidict, idna, ftfy, fsspec, frozenlist, filelock, dill, charset-normalizer, attrs, async-timeout, yarl, requests, pyarrow, pandas, nvidia-cudnn-cu11, multiprocess, importlib-metadata, aiosignal, torch, responses, huggingface-hub, aiohttp, transformers, diffusers, accelerate, datasets\n",
      "Successfully installed Pillow-9.4.0 accelerate-0.15.0 aiohttp-3.8.3 aiosignal-1.3.1 async-timeout-4.0.2 attrs-22.2.0 charset-normalizer-2.1.1 datasets-2.8.0 diffusers-0.11.1 dill-0.3.6 filelock-3.9.0 frozenlist-1.3.3 fsspec-2022.11.0 ftfy-6.1.1 huggingface-hub-0.11.1 idna-3.4 importlib-metadata-6.0.0 multidict-6.0.4 multiprocess-0.70.14 numpy-1.24.1 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 pandas-1.5.2 pyarrow-10.0.1 pytz-2022.7 pyyaml-6.0 regex-2022.10.31 requests-2.28.1 responses-0.18.0 tokenizers-0.13.2 torch-1.13.1 tqdm-4.64.1 transformers-4.25.1 typing-extensions-4.4.0 urllib3-1.26.13 xxhash-3.2.0 yarl-1.8.2 zipp-3.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U diffusers transformers accelerate ftfy datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6317cdd1-c3c2-4eac-948b-0072c3ffae77",
   "metadata": {},
   "source": [
    "If you're running on Kaggle, you'll need to install the latest PyTorch version to work with ğŸ¤— Accelerate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac2f25ea-e8aa-4b76-bf75-2266d2cc4edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu116\n",
      "Requirement already satisfied: torch in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (1.13.1)\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu116/torch-1.13.1%2Bcu116-cp38-cp38-linux_x86_64.whl (1977.9 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 GB\u001b[0m \u001b[31m471.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu116/torchvision-0.14.1%2Bcu116-cp38-cp38-linux_x86_64.whl (24.2 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu116/torchaudio-0.13.1%2Bcu116-cp38-cp38-linux_x86_64.whl (4.2 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: requests in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: numpy in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from torchvision) (1.24.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from requests->torchvision) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from requests->torchvision) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from requests->torchvision) (3.4)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.13.1\n",
      "    Uninstalling torch-1.13.1:\n",
      "      Successfully uninstalled torch-1.13.1\n",
      "Successfully installed torch-1.13.1+cu116 torchaudio-0.13.1+cu116 torchvision-0.14.1+cu116\n"
     ]
    }
   ],
   "source": [
    "# Uncomment and run if using Kaggle's notebooks. You may need to restart the notebook afterwards\n",
    "!pip install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "#!pip3 install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cu116\n",
    "#!conda install pytorch==1.12.1 torchvision==0.13.1 torchaudio==0.12.1 cudatoolkit=11.3 -c pytorch\n",
    "#!pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2f32527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.36.0.post2-py3-none-any.whl (76.3 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.3/76.3 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.36.0.post2\n"
     ]
    }
   ],
   "source": [
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a37256b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.0.4-py3-none-any.whl (137 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m137.8/137.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: traitlets>=4.3.1 in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from ipywidgets) (5.7.1)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from ipywidgets) (6.19.2)\n",
      "Collecting jupyterlab-widgets~=3.0\n",
      "  Downloading jupyterlab_widgets-3.0.5-py3-none-any.whl (384 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m384.3/384.3 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting widgetsnbextension~=4.0\n",
      "  Downloading widgetsnbextension-4.0.5-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: ipython>=6.1.0 in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from ipywidgets) (8.7.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: packaging in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (22.0)\n",
      "Requirement already satisfied: comm>=0.1.1 in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: psutil in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.0)\n",
      "Requirement already satisfied: debugpy>=1.0 in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.1)\n",
      "Requirement already satisfied: tornado>=6.1 in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.2)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (7.4.8)\n",
      "Requirement already satisfied: pyzmq>=17 in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (23.2.0)\n",
      "Requirement already satisfied: nest-asyncio in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.6)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.11.2)\n",
      "Requirement already satisfied: pickleshare in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.11 in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.36)\n",
      "Requirement already satisfied: backcall in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: stack-data in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: decorator in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: jupyter-core>=4.9.2 in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: entrypoints in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (0.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from prompt-toolkit<3.1.0,>=3.0.11->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: asttokens in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: executing in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from jupyter-core>=4.9.2->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.5.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (1.16.0)\n",
      "Installing collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.0.4 jupyterlab-widgets-3.0.5 widgetsnbextension-4.0.5\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b9adfd-4fb8-43ff-93e7-63b9dfada587",
   "metadata": {},
   "source": [
    "To be able to push your model to the Hub and make it appear on the [DreamBooth Leaderboard](https://huggingface.co/spaces/dreambooth-hackathon/leaderboard), there are a few more steps to follow. First you have to create an [access token](https://huggingface.co/docs/hub/security-tokens) with _**write access**_ from your Hugging Face account and then execute the following cell and input your token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f9624fe-b546-4c2d-9abe-26b7e085c8ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "958a515af5d54152af391574688bee82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "token = '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'\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a434b1ed-8f2f-41e7-9ff9-d752c0234d7c",
   "metadata": {},
   "source": [
    "The final step is to install Git LFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e7be7f9-e32b-41d3-9414-fdd093e2bae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!sudo apt -qq install git-lfs\n",
    "!git config --global credential.helper store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffa76b1-d323-451f-8f6d-74693a43d9d7",
   "metadata": {},
   "source": [
    "## Step 2: Pick a theme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973caaee-bceb-499f-ab34-abbece874b4e",
   "metadata": {},
   "source": [
    "This competition is composed of 5 _themes_, where each theme will collect models belong to the following categories:\n",
    "\n",
    "* **Animal ğŸ¨:** Use this theme to generate images of your pet or favourite animal hanging out in the Acropolis, swimming, or flying in space.\n",
    "* **Science ğŸ”¬:** Use this theme to generate cool synthetic images of galaxies, proteins, or any domain of the natural and medical sciences.\n",
    "* **Food ğŸ”:** Use this theme to tune Stable Diffusion on your favourite dish or cuisine.\n",
    "* **Landscape ğŸ”:** Use this theme to generate beautiful landscapes of your faourite mountain, lake, or garden.\n",
    "* **Wildcard ğŸ”¥:** Use this theme to go wild and create Stable Diffusion models for any category of your choosing!\n",
    "\n",
    "We'll be **giving out prizes to the top 3 most liked models per theme**, and you're encouraged to submit as many models as you want! Run the cell below to create a dropdown widget where you can select the theme you wish to submit to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc5e4a7f-de71-49c3-891b-d925de96b9e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deed6c4ca2274a5da4ce1f0838e375be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Pick a theme', options=('animal', 'science', 'food', 'landscape', 'wildcard'), value='anâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "theme = \"landscape\"\n",
    "drop_down = widgets.Dropdown(\n",
    "    options=[\"animal\", \"science\", \"food\", \"landscape\", \"wildcard\"],\n",
    "    description=\"Pick a theme\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "\n",
    "def dropdown_handler(change):\n",
    "    global theme\n",
    "    theme = change.new\n",
    "\n",
    "\n",
    "drop_down.observe(dropdown_handler, names=\"value\")\n",
    "display(drop_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f6ff4b1-471d-42c9-80c5-ef42dc925200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You've selected the landscape theme!\n"
     ]
    }
   ],
   "source": [
    "print(f\"You've selected the {theme} theme!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3057b6-3734-49ed-8d47-b0834549ab8b",
   "metadata": {},
   "source": [
    "## Step 3: Create an image dataset and upload it to the Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3defbc3-b9a3-40c7-87dc-61f897025dce",
   "metadata": {},
   "source": [
    "Once you've picked a theme, the next step is to **create a dataset of images for that theme** and upload it to the Hugging Face Hub:\n",
    "\n",
    "* You'll need around **10-20 images of the subject** that you wish to implant in the model. These can be photos you've taken or downloaded from platforms like [Unsplash](https://unsplash.com/). Alternatively, you can take a look at any of the [image datasets](https://huggingface.co/datasets?task_categories=task_categories:image-classification&sort=downloads) on the Hugging Face Hub for inspiration.\n",
    "* For best results, we recommend using images of your subject from **different angles and perspectives** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1e679c-2669-47aa-9f96-fa833de979c6",
   "metadata": {},
   "source": [
    "Once you've collected your images in a folder, you can upload them to the Hub by using the UI to drag and drop your images. See [this guide](https://huggingface.co/docs/datasets/upload_dataset#upload-with-the-hub-ui) for more details, or watch the video below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799652fe-b7d7-4fd4-a487-557add7549e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "YouTubeVideo(\"HaN6qCr_Afc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ef3d21-6e0e-46c9-a459-8a2ab856a5ca",
   "metadata": {},
   "source": [
    "Alternatively, you can load your dataset locally using the `imagefolder` feature of ğŸ¤— Datasets and then push it to the Hub:\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imagefolder\", data_dir=\"your_folder_of_images\")\n",
    "# Remove the dummy label column\n",
    "dataset = dataset.remove_columns(\"label\")\n",
    "# Push to Hub\n",
    "dataset.push_to_hub(\"dreambooth-hackathon-images\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb4c34c-eaa6-450f-80bc-fca10c4a08e0",
   "metadata": {},
   "source": [
    "Once you've created your dataset, you can download it by using the `load_dataset()` function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd945db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# dataset = load_dataset('imagefolder', data_dir='images/fjords')\n",
    "# dataset.push_to_hub('fjord-images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "104189cb-eb65-4058-a1a3-2678aa0b7540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 377/377 [00:00<00:00, 346kB/s]\n",
      "Using custom data configuration StatsGary--dreambooth-hackathon-images-6785c60332277b2d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None to /home/gary_hutson/.cache/huggingface/datasets/StatsGary___parquet/StatsGary--dreambooth-hackathon-images-6785c60332277b2d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150M/150M [00:01<00:00, 84.2MB/s]\n",
      "Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.64s/it]\n",
      "Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 1093.12it/s]\n",
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/gary_hutson/.cache/huggingface/datasets/StatsGary___parquet/StatsGary--dreambooth-hackathon-images-6785c60332277b2d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image'],\n",
       "    num_rows: 28\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_id = \"StatsGary/dreambooth-hackathon-images\"  # CHANGE THIS TO YOUR {hub_username}/{dataset_id}\n",
    "dataset = load_dataset(dataset_id, split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bceca53-a9af-492a-98db-006bff3819fb",
   "metadata": {},
   "source": [
    "Now that we have our dataset, let's define a helper function to view a few of the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a23ebf-cf7f-4acf-afd9-fabafa045b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "\n",
    "\n",
    "# def image_grid(imgs, rows, cols):\n",
    "#     assert len(imgs) == rows * cols\n",
    "#     w, h = imgs[0].size\n",
    "#     grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n",
    "#     grid_w, grid_h = grid.size\n",
    "#     for i, img in enumerate(imgs):\n",
    "#         grid.paste(img, box=(i % cols * w, i // cols * h))\n",
    "#     return grid\n",
    "\n",
    "\n",
    "# num_samples = 4\n",
    "# image_grid(dataset[\"image\"][:num_samples], rows=1, cols=num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f1018f-ab7d-432c-b577-a5c606e3362b",
   "metadata": {},
   "source": [
    "If this looks good, you can move onto the next step - creating a PyTorch dataset for training with DreamBooth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d553ed-fe74-43ca-91b3-6442fec00dda",
   "metadata": {},
   "source": [
    "## Step 3: Create a training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9f2811-2b00-4687-b9b4-8551db8b130f",
   "metadata": {},
   "source": [
    "To create a training set for our images we need a few components:\n",
    "\n",
    "* An _instance prompt_ that is used to prime the model at the start of training. In most cases, using \"a photo of [identifier] [class noun]\" works quite well, e.g. \"a photo of ccorgi dog\" for our cute Corgi pictures. \n",
    "    * **Note:** it is recommended that you pick a unique / made up word like `ccorgi` to describe your subject. This will ensure a common word in the model's vocabulary isn't overwritten.\n",
    "* A _tokenizer_ to convert the instance prompt into input IDs that can be fed to the text encoder of Stable Diffusion\n",
    "* A set of _image transforms_, notably resizing the images to a common shape and normalizing the pixel values to a common mean and standard distribution.\n",
    "\n",
    "With this in mind, let's start by defining the instance prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "745d51d6-962d-47ec-a1b5-b30e7b2e024c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance prompt: a photo of norweigen-fjords fjords\n"
     ]
    }
   ],
   "source": [
    "name_of_your_concept = \"norweigen-fjords\"  # CHANGE THIS ACCORDING TO YOUR SUBJECT\n",
    "type_of_thing = \"fjords\"  # CHANGE THIS ACCORDING TO YOUR SUBJECT\n",
    "instance_prompt = f\"a photo of {name_of_your_concept} {type_of_thing}\"\n",
    "print(f\"Instance prompt: {instance_prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a5b07c-11d9-4d24-aa07-ffbeff058f7b",
   "metadata": {},
   "source": [
    "Next, we need to create a PyTorch `Dataset` object that implements the `__len__` and `__getitem__` dunder methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "605a9a45-cab4-4b34-b411-ba924e508f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class DreamBoothDataset(Dataset):\n",
    "    def __init__(self, dataset, instance_prompt, tokenizer, size=512):\n",
    "        self.dataset = dataset\n",
    "        self.instance_prompt = instance_prompt\n",
    "        self.tokenizer = tokenizer\n",
    "        self.size = size\n",
    "        self.transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(size),\n",
    "                transforms.CenterCrop(size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = {}\n",
    "        image = self.dataset[index][\"image\"]\n",
    "        example[\"instance_images\"] = self.transforms(image)\n",
    "        example[\"instance_prompt_ids\"] = self.tokenizer(\n",
    "            self.instance_prompt,\n",
    "            padding=\"do_not_pad\",\n",
    "            truncation=True,\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "        ).input_ids\n",
    "        return example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391a7f3b-4565-486a-986d-c745249426f1",
   "metadata": {},
   "source": [
    "Great, let's now check this works by loading the CLIP tokenizer associated with the text encoder of the original Stable Diffusion model, and then creating the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b139c7fe-eec2-43fc-94da-ac4ecaae73f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.06M/1.06M [00:00<00:00, 7.69MB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 525k/525k [00:00<00:00, 6.65MB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 472/472 [00:00<00:00, 429kB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 806/806 [00:00<00:00, 809kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'instance_images': tensor([[[ 0.8196,  0.8275,  0.8275,  ...,  0.2078,  0.2078,  0.2157],\n",
       "          [ 0.8196,  0.8196,  0.8353,  ...,  0.2078,  0.2157,  0.2078],\n",
       "          [ 0.8196,  0.8275,  0.8275,  ...,  0.2078,  0.2078,  0.1922],\n",
       "          ...,\n",
       "          [ 0.2157,  0.2314,  0.2471,  ..., -0.7255, -0.7333, -0.7412],\n",
       "          [ 0.2627,  0.2627,  0.2706,  ..., -0.7333, -0.7333, -0.7333],\n",
       "          [ 0.2000,  0.2078,  0.2078,  ..., -0.7333, -0.7333, -0.7333]],\n",
       " \n",
       "         [[ 0.9137,  0.9137,  0.9137,  ...,  0.5529,  0.5529,  0.5529],\n",
       "          [ 0.9137,  0.9216,  0.9137,  ...,  0.5451,  0.5451,  0.5451],\n",
       "          [ 0.9137,  0.9137,  0.9137,  ...,  0.5451,  0.5451,  0.5373],\n",
       "          ...,\n",
       "          [ 0.3882,  0.3961,  0.3961,  ..., -0.7255, -0.7333, -0.7255],\n",
       "          [ 0.3882,  0.3882,  0.3804,  ..., -0.7333, -0.7333, -0.7255],\n",
       "          [ 0.3647,  0.3647,  0.3647,  ..., -0.7333, -0.7333, -0.7333]],\n",
       " \n",
       "         [[ 1.0000,  1.0000,  1.0000,  ...,  0.9843,  0.9843,  0.9843],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  0.9843,  0.9765,  0.9843],\n",
       "          [ 1.0000,  1.0000,  1.0000,  ...,  0.9843,  0.9686,  0.9765],\n",
       "          ...,\n",
       "          [ 0.6235,  0.6235,  0.6314,  ..., -0.7255, -0.7333, -0.7333],\n",
       "          [ 0.6235,  0.6157,  0.6157,  ..., -0.7255, -0.7333, -0.7333],\n",
       "          [ 0.6078,  0.6000,  0.6000,  ..., -0.7255, -0.7333, -0.7333]]]),\n",
       " 'instance_prompt_ids': [49406,\n",
       "  320,\n",
       "  1125,\n",
       "  539,\n",
       "  14414,\n",
       "  33070,\n",
       "  268,\n",
       "  31260,\n",
       "  646,\n",
       "  31260,\n",
       "  646,\n",
       "  49407]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import CLIPTokenizer\n",
    "\n",
    "# The Stable Diffusion checkpoint we'll fine-tune\n",
    "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    subfolder=\"tokenizer\",\n",
    ")\n",
    "\n",
    "train_dataset = DreamBoothDataset(dataset, instance_prompt, tokenizer)\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2493c90-9a9a-4843-9072-d4a7d6b626a8",
   "metadata": {},
   "source": [
    "## Step 4: Define a data collator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc114774-6afd-496b-ba16-e9b505d8d8e1",
   "metadata": {},
   "source": [
    "Now that we have a training dataset, the next thing we need is to define a _data collator_. A data collator is a function that collects elements in a batch of data and applies some logic to form a single tensor we can provide to the model. If you'd to learn more, you can check out this video from the [Hugging Face Course](hf.co/course):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2e54ac-1cf8-417d-af90-9f460ae394b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo(\"-RPeakdlHYo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a1e4a9-41fc-4e06-a072-51665071aee2",
   "metadata": {},
   "source": [
    "For DreamBooth, our data collator need to provide the model with the input IDs from the tokenizer and the pixel values from the images as a stacked tensor. The function below does the trick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93cfffaa-7f94-4d11-bee5-6f1d3f7d6961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def collate_fn(examples):\n",
    "    input_ids = [example[\"instance_prompt_ids\"] for example in examples]\n",
    "    pixel_values = [example[\"instance_images\"] for example in examples]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "\n",
    "    input_ids = tokenizer.pad(\n",
    "        {\"input_ids\": input_ids}, padding=True, return_tensors=\"pt\"\n",
    "    ).input_ids\n",
    "\n",
    "    batch = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"pixel_values\": pixel_values,\n",
    "    }\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98941c88-2678-43ac-bb0e-8e1842322892",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 5: Load the components of the Stable Diffusion pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8719ea-235b-4dea-9c99-cb320fd77f2e",
   "metadata": {},
   "source": [
    "We nearly have all the pieces ready for training! As you saw in the Unit 3 notebook on Stable Diffusion, the pipeline is composed of several models:\n",
    "\n",
    "* A text encoder that converts the prompts into text embeddings. Here we're using CLIP since it's the encoder used to train Stable Diffusion v1-4\n",
    "* A VAE or variational autoencoder that converts the images to compressed representations (i.e. latents) and decompresses them at inference time\n",
    "* A UNet that applies the denoising operation on the latent of the VAE\n",
    "\n",
    "We can load all these components using the ğŸ¤— Diffusers and ğŸ¤— Transformers libraries as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e7cdb66-3f2b-46d8-9069-2187d652d964",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 592/592 [00:00<00:00, 512kB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 492M/492M [00:05<00:00, 91.3MB/s] \n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 335M/335M [00:04<00:00, 82.4MB/s] \n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 522/522 [00:00<00:00, 456kB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.44G/3.44G [00:38<00:00, 89.7MB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 743/743 [00:00<00:00, 616kB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 316/316 [00:00<00:00, 207kB/s]\n"
     ]
    }
   ],
   "source": [
    "from diffusers import AutoencoderKL, UNet2DConditionModel\n",
    "from transformers import CLIPFeatureExtractor, CLIPTextModel\n",
    "\n",
    "text_encoder = CLIPTextModel.from_pretrained(model_id, subfolder=\"text_encoder\")\n",
    "vae = AutoencoderKL.from_pretrained(model_id, subfolder=\"vae\")\n",
    "unet = UNet2DConditionModel.from_pretrained(model_id, subfolder=\"unet\")\n",
    "feature_extractor = CLIPFeatureExtractor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5897f28-8ea0-4ae1-acde-42b0016cc153",
   "metadata": {},
   "source": [
    "## Step 6: Fine-tune the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7691f770-7c96-493e-b462-f8aa7bfc7932",
   "metadata": {},
   "source": [
    "Now comes the fun part - training our model with DreamBooth! As shown in the [Hugging Face's blog post](https://huggingface.co/blog/dreambooth), the most essential hyperparameters to tweak are the learning rate and number of training steps.\n",
    "\n",
    "In general, you'll get better results with a lower learning rate at the expense of needing to increase the number of training steps. The values below are a good starting point, but you may need to adjust them according to your dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ad4ebf3-ff54-41a9-8094-f961c1abeda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 2e-06\n",
    "max_train_steps = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038d0606-c761-4660-81da-23d24f089fe9",
   "metadata": {},
   "source": [
    "Next, let's wrap the other hyperparameters we need in a `Namespace` object to make it easier to configure the training run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22c21df7-8cdd-45cc-97b5-3f2b4362f952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "args = Namespace(\n",
    "    pretrained_model_name_or_path=model_id,\n",
    "    resolution=512, # Reduce this if 512 you want to save some memory\n",
    "    train_dataset=train_dataset,\n",
    "    instance_prompt=instance_prompt,\n",
    "    learning_rate=learning_rate,\n",
    "    max_train_steps=max_train_steps,\n",
    "    train_batch_size=1,\n",
    "    gradient_accumulation_steps=10, # Increase this if you want to lower memory usage\n",
    "    max_grad_norm=1.0,\n",
    "    gradient_checkpointing=True,  # set this to True to lower the memory usage.\n",
    "    use_8bit_adam=True,  # use 8bit optimizer from bitsandbytes\n",
    "    seed=3434554,\n",
    "    sample_batch_size=2, #Normally 2\n",
    "    output_dir=\"norweigen-fjords-dreambooth\",  # where to save the pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79571e6d-e392-4418-8a33-d3744f3e09aa",
   "metadata": {},
   "source": [
    "The final step is to define a `training_function()` function that wraps the training logic and can be passed to ğŸ¤— Accelerate to handle training on 1 or more GPUs. If this is the first time you're using ğŸ¤— Accelerate, check out this video to get a quick overview of what it can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1f1734-4bf6-4369-95cd-ae484bf29971",
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo(\"s7dy8QRgjJ0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ae3a83-8f42-42ad-b9e4-ec1cc48691ac",
   "metadata": {},
   "source": [
    "The details should look familiar to what we saw in Units 1 & 2 when we trained our own diffusion models from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a0d36a0-ad17-415e-bf6b-7a0a96928649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "from diffusers import DDPMScheduler, PNDMScheduler, StableDiffusionPipeline\n",
    "from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def training_function(text_encoder, vae, unet):\n",
    "\n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "    )\n",
    "\n",
    "    set_seed(args.seed)\n",
    "\n",
    "    if args.gradient_checkpointing:\n",
    "        unet.enable_gradient_checkpointing()\n",
    "\n",
    "    # Use 8-bit Adam for lower memory usage or to fine-tune the model in 16GB GPUs\n",
    "    if args.use_8bit_adam:\n",
    "        import bitsandbytes as bnb\n",
    "        optimizer_class = bnb.optim.AdamW8bit\n",
    "    else:\n",
    "        optimizer_class = torch.optim.AdamW\n",
    "\n",
    "    optimizer = optimizer_class(\n",
    "        unet.parameters(),  # only optimize unet\n",
    "        lr=args.learning_rate,\n",
    "    )\n",
    "\n",
    "    noise_scheduler = DDPMScheduler(\n",
    "        beta_start=0.00085,\n",
    "        beta_end=0.012,\n",
    "        beta_schedule=\"scaled_linear\",\n",
    "        num_train_timesteps=1000,\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        args.train_dataset,\n",
    "        batch_size=args.train_batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "    unet, optimizer, train_dataloader = accelerator.prepare(\n",
    "        unet, optimizer, train_dataloader\n",
    "    )\n",
    "\n",
    "    # Move text_encode and vae to gpu\n",
    "    text_encoder.to(accelerator.device)\n",
    "    vae.to(accelerator.device)\n",
    "\n",
    "    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
    "    num_update_steps_per_epoch = math.ceil(\n",
    "        len(train_dataloader) / args.gradient_accumulation_steps\n",
    "    )\n",
    "    num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "    # Train!\n",
    "    total_batch_size = (\n",
    "        args.train_batch_size\n",
    "        * accelerator.num_processes\n",
    "        * args.gradient_accumulation_steps\n",
    "    )\n",
    "    # Only show the progress bar once on each machine.\n",
    "    progress_bar = tqdm(\n",
    "        range(args.max_train_steps), disable=not accelerator.is_local_main_process\n",
    "    )\n",
    "    progress_bar.set_description(\"Steps\")\n",
    "    global_step = 0\n",
    "\n",
    "    for epoch in range(num_train_epochs):\n",
    "        unet.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            with accelerator.accumulate(unet):\n",
    "                # Convert images to latent space\n",
    "                with torch.no_grad():\n",
    "                    latents = vae.encode(batch[\"pixel_values\"]).latent_dist.sample()\n",
    "                    latents = latents * 0.18215\n",
    "\n",
    "                # Sample noise that we'll add to the latents\n",
    "                noise = torch.randn(latents.shape).to(latents.device)\n",
    "                bsz = latents.shape[0]\n",
    "                # Sample a random timestep for each image\n",
    "                timesteps = torch.randint(\n",
    "                    0,\n",
    "                    noise_scheduler.config.num_train_timesteps,\n",
    "                    (bsz,),\n",
    "                    device=latents.device,\n",
    "                ).long()\n",
    "\n",
    "                # Add noise to the latents according to the noise magnitude at each timestep\n",
    "                # (this is the forward diffusion process)\n",
    "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "                # Get the text embedding for conditioning\n",
    "                with torch.no_grad():\n",
    "                    encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
    "\n",
    "                # Predict the noise residual\n",
    "                noise_pred = unet(\n",
    "                    noisy_latents, timesteps, encoder_hidden_states\n",
    "                ).sample\n",
    "                loss = (\n",
    "                    F.mse_loss(noise_pred, noise, reduction=\"none\")\n",
    "                    .mean([1, 2, 3])\n",
    "                    .mean()\n",
    "                )\n",
    "\n",
    "                accelerator.backward(loss)\n",
    "                if accelerator.sync_gradients:\n",
    "                    accelerator.clip_grad_norm_(unet.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "            if accelerator.sync_gradients:\n",
    "                progress_bar.update(1)\n",
    "                global_step += 1\n",
    "\n",
    "            logs = {\"loss\": loss.detach().item()}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "\n",
    "            if global_step >= args.max_train_steps:\n",
    "                break\n",
    "\n",
    "        accelerator.wait_for_everyone()\n",
    "\n",
    "    # Create the pipeline using using the trained modules and save it.\n",
    "    if accelerator.is_main_process:\n",
    "        print(f\"Loading pipeline and saving to {args.output_dir}...\")\n",
    "        scheduler = PNDMScheduler(\n",
    "            beta_start=0.00085,\n",
    "            beta_end=0.012,\n",
    "            beta_schedule=\"scaled_linear\",\n",
    "            skip_prk_steps=True,\n",
    "            steps_offset=1,\n",
    "        )\n",
    "        pipeline = StableDiffusionPipeline(\n",
    "            text_encoder=text_encoder,\n",
    "            vae=vae,\n",
    "            unet=accelerator.unwrap_model(unet),\n",
    "            tokenizer=tokenizer,\n",
    "            scheduler=scheduler,\n",
    "            safety_checker=StableDiffusionSafetyChecker.from_pretrained(\n",
    "                \"CompVis/stable-diffusion-safety-checker\"\n",
    "            ),\n",
    "            feature_extractor=feature_extractor,\n",
    "        )\n",
    "        pipeline.save_pretrained(args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cbd535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch\n",
    "# !pip install --quiet bitsandbytes\n",
    "# !pip install --quiet git+https://github.com/huggingface/transformers.git \n",
    "# !pip install --quiet accelerate\n",
    "# !pip install --quiet sentencepiece\n",
    "# !conda install -y cudatoolkit\n",
    "# !find /opt/conda/ -name libcudart.so\n",
    "# import os\n",
    "# os.environ['LD_LIBRARY_PATH'] = '/opt/conda/lib/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348b6040-200e-407f-a0bb-3902b72b7e7b",
   "metadata": {},
   "source": [
    "Now that we have the function defined, let's train it! Depending on the size of your dataset and type of GPU, this can take anywhere from 5 minutes to 1 hour to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0e764c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# git clone git@github.com:TimDettmers/bitsandbytes.git\n",
    "# cd bitsandbytes\n",
    "# CUDA_VERSION=1\n",
    "# python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44d383c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f77d9fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: bitsandbytes 0.35.4\n",
      "Uninstalling bitsandbytes-0.35.4:\n",
      "  Successfully uninstalled bitsandbytes-0.35.4\n"
     ]
    }
   ],
   "source": [
    "#!pip uninstall bitsandbytes -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ce41655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: can't open file '/home/gary_hutson/GitHub/diffusion_models/Hackathon/setup.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "#!git clone https://github.com/TimDettmers/bitsandbytes.git\n",
    "#!export LD_LIBRARY_PATH=/usr/local/cuda/lib64/:$LD_LIBRARY_PATH\n",
    "#!cd bitsandbytes\n",
    "#!CUDA_VERSION=120 \n",
    "#!python setup.py install\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "945b54b1-9e44-49f6-b948-08fef24670f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on one GPU.\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "For effortless bug reporting copy-paste your error into this form: https://docs.google.com/forms/d/e/1FAIpQLScPB8emS3Thkp66nvqwmjTEgxp8Y9ufuWTzFyr9kJ5AoI47dQ/viewform?usp=sf_link\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:134: UserWarning: /home/gary_hutson/anaconda3 did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:134: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:134: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
      "CUDA SETUP: Detected CUDA version 120\n",
      "CUDA SETUP: Loading binary /home/gary_hutson/anaconda3/envs/diffusion/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda120.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   0%|          | 1/400 [00:16<1:50:13, 16.58s/it, loss=0.0358]"
     ]
    }
   ],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "num_of_gpus = 1 # CHANGE THIS TO MATCH THE NUMBER OF GPUS YOU HAVE\n",
    "notebook_launcher(\n",
    "    training_function, args=(text_encoder, vae, unet), num_processes=num_of_gpus\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "61a6c8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0367a05d-a75a-4e9d-9b0d-64485c49b561",
   "metadata": {},
   "source": [
    "If you're running on a single GPU, you can free up some memory for the next section by copying the code below into a new cell and running it. For multi-GPU machines, ğŸ¤— Accelerate doesn't allow _any_ cell to directly access the GPU with `torch.cuda`, so we don't recommend using this trick in those cases:\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0a6c7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Dec 30 13:15:51 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.60.13    Driver Version: 525.60.13    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            On   | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   68C    P0    32W /  70W |  14940MiB / 15360MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1477      G   /usr/lib/xorg/Xorg                 95MiB |\n",
      "|    0   N/A  N/A      1517      G   /usr/bin/gnome-shell                7MiB |\n",
      "|    0   N/A  N/A     14405      C   .../envs/diffpy39/bin/python    14832MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a898b536-71e3-4638-8fa9-e1d266ce9a4c",
   "metadata": {},
   "source": [
    "## Step 7: Run inference and inspect generations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64cc02b-bc7b-4e7e-9125-9bb39fc6440c",
   "metadata": {},
   "source": [
    "Now that we've trained the model, let's generate some images with it to see how it fares! First we'll load the pipeline from the output directory we save the model to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef24b026-d307-4ad1-833b-5195f1d97e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    args.output_dir,\n",
    "    torch_dtype=torch.float16,\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfa9d82-5d05-444b-8f4e-f00119717014",
   "metadata": {},
   "source": [
    "Next, let's generate a few images. The `prompt` variable will later be used to set the default on the Hugging Face Hub widget, so experiment a bit to find a good one. You might also want to try creating elaborate prompts with [CLIP Interrogator](https://huggingface.co/spaces/pharma/CLIP-Interrogator):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e23cd8-adbc-49ef-b094-423bbde30487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a funny prompt here and it will be used as the widget's default \n",
    "# when we push to the Hub in the next section\n",
    "prompt = f\"a photo of {name_of_your_concept} {type_of_thing} in the Acropolis\"\n",
    "\n",
    "# Tune the guidance to control how closely the generations follow the prompt.\n",
    "# Values between 7-11 usually work best\n",
    "guidance_scale = 7\n",
    "\n",
    "num_cols = 2\n",
    "all_images = []\n",
    "for _ in range(num_cols):\n",
    "    images = pipe(prompt, guidance_scale=guidance_scale).images\n",
    "    all_images.extend(images)\n",
    "\n",
    "image_grid(all_images, 1, num_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4991d265-d411-4f48-9f2f-525b34406b85",
   "metadata": {},
   "source": [
    "## Step 8: Push your model to the Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fbd090-ffe5-44bc-81b8-e938eff7cb84",
   "metadata": {},
   "source": [
    "If you're happy with you model, the final step is to push it to the Hub and view it on the [DreamBooth Leaderboard](https://huggingface.co/spaces/dreambooth-hackathon/leaderboard)!\n",
    "\n",
    "First, you'll need to define a name for your model repo. By default, we use the unique identifier and class name, but feel free to change this if you want: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884829cf-7e52-4bcc-848f-a82e27e800e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a name for your model on the Hub. No spaces allowed.\n",
    "model_name = f\"{name_of_your_concept}-{type_of_thing}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190b4516-7ed1-4c36-b0bb-ded6e6edfdba",
   "metadata": {},
   "source": [
    "Next, add a brief description on the type of model you've trained or any other information you'd like to share:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ab40e0-f8ad-4ae2-8b28-8afc996f7dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the theme and model you've trained\n",
    "description = f\"\"\"\n",
    "This is a Stable Diffusion model fine-tuned on `{type_of_thing}` images for the {theme} theme.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86abb6cf-c2e4-4ae9-bd78-c71e2ad0026c",
   "metadata": {},
   "source": [
    "Finally, run the cell below to create a repo on the Hub and push all our files with a nice model card to boot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ee936c-be6c-4f7a-80e0-de05f78c604b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to upload a pipeline saved locally to the hub\n",
    "from huggingface_hub import HfApi, ModelCard, create_repo, get_full_repo_name\n",
    "\n",
    "# Set up repo and upload files\n",
    "hub_model_id = get_full_repo_name(model_name)\n",
    "create_repo(hub_model_id)\n",
    "api = HfApi()\n",
    "api.upload_folder(folder_path=args.output_dir, path_in_repo=\"\", repo_id=hub_model_id)\n",
    "\n",
    "content = f\"\"\"\n",
    "---\n",
    "license: creativeml-openrail-m\n",
    "tags:\n",
    "- pytorch\n",
    "- diffusers\n",
    "- stable-diffusion\n",
    "- text-to-image\n",
    "- diffusion-models-class\n",
    "- dreambooth-hackathon\n",
    "- {theme}\n",
    "widget:\n",
    "- text: {prompt}\n",
    "---\n",
    "\n",
    "# DreamBooth model for the {name_of_your_concept} concept trained by {api.whoami()[\"name\"]} on the {dataset_id} dataset.\n",
    "\n",
    "This is a Stable Diffusion model fine-tuned on the {name_of_your_concept} concept with DreamBooth. It can be used by modifying the `instance_prompt`: **{instance_prompt}**\n",
    "\n",
    "This model was created as part of the DreamBooth Hackathon ğŸ”¥. Visit the [organisation page](https://huggingface.co/dreambooth-hackathon) for instructions on how to take part!\n",
    "\n",
    "## Description\n",
    "\n",
    "{description}\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "pipeline = StableDiffusionPipeline.from_pretrained('{hub_model_id}')\n",
    "image = pipeline().images[0]\n",
    "image\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "card = ModelCard(content)\n",
    "hub_url = card.push_to_hub(hub_model_id)\n",
    "print(f\"Upload successful! Model can be found here: {hub_url}\")\n",
    "print(\n",
    "    f\"View your submission on the public leaderboard here: https://huggingface.co/spaces/dreambooth-hackathon/leaderboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbba5a20-9d70-4efc-b823-b560fe34b822",
   "metadata": {},
   "source": [
    "## Step 9: Celebrate ğŸ¥³\n",
    "\n",
    "Congratulations, you've trained your very first DreamBooth model! You can train as many models as you want for the competition - the important thing is that **the most liked models will win prizes** so don't forget to share your creation far and wide to get the most votes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22e8a3e-620e-4dff-8cea-3732003c17fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "c92386b37a3e09cd06dce39e5fefe1e4fc24666937b3caebcb37fb03c4f266c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
